{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af978a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import time\n",
    "block_size=8\n",
    "batch_size = 4\n",
    "max_iters = 1000000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe5d853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712523122.936298\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365fd013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '&', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt','r',encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "chars=sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size=len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989de438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4156ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([46, 62, 59,  1, 42, 72, 69, 64, 59, 57, 74,  1, 33, 75, 74, 59, 68, 56,\n",
      "        59, 72, 61,  1, 59, 28, 69, 69, 65,  1, 69, 60,  1, 46, 62, 59,  1, 49,\n",
      "        69, 68, 58, 59, 72, 60, 75, 66,  1, 49, 63, 80, 55, 72, 58,  1, 69, 60,\n",
      "         1, 41, 80,  0,  0,  0, 46, 63, 74, 66, 59, 24,  1, 46, 62, 59,  1, 49,\n",
      "        69, 68, 58, 59, 72, 60, 75, 66,  1, 49, 63, 80, 55, 72, 58,  1, 69, 60,\n",
      "         1, 41, 80,  0,  0,  0, 27, 75, 74, 62])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) #This line converts the input text into a tensor of long integers. \n",
    "#The encode function is responsible for converting the text into a sequence of numerical tokens.\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f449a891",
   "metadata": {},
   "source": [
    "A bigram model predicts the next token based on the current token and the previous token.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f04e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([46, 62, 59,  ...,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c573d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[ 1, 77, 59, 66, 66,  1, 69, 63],\n",
      "        [77, 55, 73,  1, 74, 62, 59,  1],\n",
      "        [74, 75, 60, 60, 59, 58,  1, 77],\n",
      "        [55, 58,  0, 77, 55, 66, 65, 59]])\n",
      "targets:\n",
      "tensor([[77, 59, 66, 66,  1, 69, 63, 66],\n",
      "        [55, 73,  1, 74, 62, 59,  1, 62],\n",
      "        [75, 60, 60, 59, 58,  1, 77, 63],\n",
      "        [58,  0, 77, 55, 66, 65, 59, 58]])\n"
     ]
    }
   ],
   "source": [
    "#This line calculates the index at which the dataset should be split into training and validation sets. \n",
    "#In this case, 80% of the data is used for training.\n",
    "n = int(0.8*len(data))\n",
    "#These lines split the data into training and validation sets based on the index n calculated in the previous step\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81f73f4e",
   "metadata": {},
   "source": [
    "# Initialize an embedding layer\n",
    "vocab_size = 1000 #This variable represents the size of the vocabulary, i.e., the total number of unique words or tokens in your dataset.\n",
    "embedding_dim = 100 #Each word in the vocabulary will be represented by a vector of length 100 in the embedding space\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)# initializes an embedding layer using nn.Embedding. It takes two arguments: vocab_size and embedding_dim. #This layer will map each word index (integer) to its corresponding embedding vector.\n",
    "\n",
    "# Create some input indices #ine creates a tensor containing the indices of the words to be embedded. These indices represent the positions of the words in the vocabulary. For example, 1 might represent the index of the word \"hello,\" 5 might represent the index of the word \"world,\" and so on.\n",
    "input_indices = torch.LongTensor([1, 5, 3, 2])\n",
    "\n",
    "# Apply the embedding layer #performs the embedding lookup operation. It takes the input indices tensor and returns the corresponding embedding vectors for each word index. \n",
    "#The resulting tensor embedded_output will have a shape of (4, 100) because there are 4 input indices and each embedding vector has a dimensionality of 100.\n",
    "embedded_output = embedding(input_indices)\n",
    "\n",
    "# The output will be a tensor of shape (4, 100), where 4 is the number of inputs\n",
    "# and 100 is the dimensionality of the embedding vectors\n",
    "print(embedded_output.shape)\n",
    "#print(embedded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788f020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a94e6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for only evaluation of models and not training.\n",
    "@torch.no_grad() # This is a Python decorator used in PyTorch to temporarily disable gradient computation. This is useful when you're only interested in inference, as it reduces memory usage and speeds up computation. Inside the decorated function, gradients won't be tracked, saving computational resources.\n",
    "def estimate_loss(): # defines the estimate_loss function without any parameters.\n",
    "    out = {}  #empty dictionary named out which will store the calculated losses.\n",
    "    model.eval() #puts the model into evaluation mode. By calling model.eval(), you're indicating that you're not going to perform any training and therefore want the model to operate in evaluation mode.\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) # initializes a tensor named losses filled with zeros, with length 'eval_iters'.\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y) #This line passes the input (X) and target (Y) data through the model (model). It computes the logits (raw output) and the loss using the model's forward pass.\n",
    "            losses[k] = loss.item() #stores the loss value (converted to a Python float using '.item()') in the losses tensor at index k.\n",
    "        out[split] = losses.mean() # This calculates the mean loss across all iterations (eval_iters) and stores it in the out dictionary with the key being the current split ('train' or 'val').\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fadf6bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jp)q6x“FmakYJ1.cqYS3g-NP-/VSE”;YJq,RUEB*f[-ayavand0ioK“gbi(0‘ai(\n",
      "HzS—%&iKN8/VC&5P—jklD—DB•Io;ph SrJU!e2b;?6ZF‘Lx*uLMxDzTEeGfGDPCSpyUq%imK-et#lw!h!WLdhXAFsK)%&O[Odq•cOf#Z8MBLzeGDxfKjY)Hf4jg”?3]-I4II4dM1*0eq&™tnz‘JKQY.vz™hh]37Uj3ft&i4e6jg/0Of’Vt)TrnWag]•,W;4im0Gc9xC6nd”W9I*lJR 5EcfPI0[qGEA•4F7wOdnmG.D%]T%—GS)mJu—,EaCQZ6J]BAEQ,)sxck]:6fQlE-vdHDlAm7mwLOUf•‘Y”Oewh]3R•—%Xq•wi\n",
      "/yAj&0Lbq%qcgsE!m.q&6J™“NE&!5—]-QZ#%HHCQAaYgei6qc1R3zJ; wf7]dNB—”:t(D-z]j[-]/Q2;ugi(ljd“Rx*QrND\n",
      "PtUg-ArLM-TQSEQGBKe“JWg)uX*L\n",
      "rD\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):#defines a class named BigramLanguageModel that inherits from nn.Module. \n",
    "    def __init__(self, vocab_size):#It initializes with a constructor method __init__ and 'vocab_size' as a parameter. \n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)#it initializes an embedding table named token_embedding_table using nn.Embedding from PyTorch. \n",
    "\n",
    "        \n",
    " ##This part defines the forward method of the BigramLanguageModel class. This method takes index and optionally targets. \n",
    " ##Inside the method, it retrieves embeddings for the given index from the embedding table. \n",
    " ##If targets are provided, it calculates the loss using cross-entropy loss function (F.cross_entropy). \n",
    " ##The logits and loss are returned.       \n",
    "   \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "#Below part defines the generate method of the BigramLanguageModel class. \n",
    "#It generates new tokens given an initial index and the maximum number of new tokens max_new_tokens. \n",
    "#Inside the method, it iterates for max_new_tokens times. \n",
    "#At each iteration, it predicts the next token using the forward method, then samples a token from the predicted probabilities, and appends it to the running sequence. \n",
    "#Finally, it returns the updated index containing the generated tokens.\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "#Creates an instance of BigramLanguageModel named model with a specified vocabulary size (vocab_size). \n",
    "#It then moves the model to a specified device (device). It initializes a context tensor of shape (1, 1) filled with zeros on the same device. \n",
    "#Then, it generates new characters using the generate method with the given context and maximum new tokens, decodes the generated tokens, and prints them out.    \n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc9ee8d",
   "metadata": {},
   "source": [
    "About Optimizers\n",
    "\n",
    "1.Adam: Adam (Adaptive Moment Estimation) is an extension of the stochastic gradient descent algorithm. \n",
    "Adam combines the ideas of momentum and RMSprop. \n",
    "It uses a moving average of both the gradient and its squared value to adapt the learning rate of each parameter. \n",
    "Adam is often used as a default optimizer for deep learning models.\n",
    "Adam adapts the learning rate for each parameter based on estimates of the first and second moments of the gradients.\n",
    "\n",
    "2.AdamW: AdamW is a modification of the Adam optimizer that adds weight decay to the parameter updates. \n",
    "Weight decay is a regularization technique that penalizes large weights in the neural network to prevent overfitting.\n",
    "This helps to regularize the model and can improve generalization performance. \n",
    "\n",
    "\n",
    "In simpler terms, the main difference between Adam and AdamW is how they handle weight decay. AdamW separates the weight decay from the optimization process, which can lead to better regularization and potentially improved performance, especially in situations where overfitting is a concern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b84d0949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 5.057, val loss: 5.069\n",
      "step: 2500, train loss: 4.423, val loss: 4.474\n",
      "step: 5000, train loss: 3.904, val loss: 3.999\n",
      "step: 7500, train loss: 3.485, val loss: 3.609\n",
      "step: 10000, train loss: 3.171, val loss: 3.330\n",
      "step: 12500, train loss: 2.945, val loss: 3.135\n",
      "step: 15000, train loss: 2.777, val loss: 2.984\n",
      "step: 17500, train loss: 2.666, val loss: 2.890\n",
      "step: 20000, train loss: 2.586, val loss: 2.824\n",
      "step: 22500, train loss: 2.544, val loss: 2.780\n",
      "step: 25000, train loss: 2.498, val loss: 2.744\n",
      "step: 27500, train loss: 2.466, val loss: 2.727\n",
      "step: 30000, train loss: 2.432, val loss: 2.706\n",
      "step: 32500, train loss: 2.417, val loss: 2.691\n",
      "step: 35000, train loss: 2.406, val loss: 2.703\n",
      "step: 37500, train loss: 2.397, val loss: 2.680\n",
      "step: 40000, train loss: 2.386, val loss: 2.690\n",
      "step: 42500, train loss: 2.379, val loss: 2.658\n",
      "step: 45000, train loss: 2.369, val loss: 2.681\n",
      "step: 47500, train loss: 2.366, val loss: 2.669\n",
      "step: 50000, train loss: 2.357, val loss: 2.674\n",
      "step: 52500, train loss: 2.364, val loss: 2.679\n",
      "step: 55000, train loss: 2.362, val loss: 2.682\n",
      "step: 57500, train loss: 2.349, val loss: 2.689\n",
      "step: 60000, train loss: 2.355, val loss: 2.676\n",
      "step: 62500, train loss: 2.357, val loss: 2.699\n",
      "step: 65000, train loss: 2.354, val loss: 2.688\n",
      "step: 67500, train loss: 2.355, val loss: 2.684\n",
      "step: 70000, train loss: 2.346, val loss: 2.702\n",
      "step: 72500, train loss: 2.345, val loss: 2.710\n",
      "step: 75000, train loss: 2.350, val loss: 2.700\n",
      "step: 77500, train loss: 2.347, val loss: 2.692\n",
      "step: 80000, train loss: 2.342, val loss: 2.701\n",
      "step: 82500, train loss: 2.350, val loss: 2.697\n",
      "step: 85000, train loss: 2.342, val loss: 2.713\n",
      "step: 87500, train loss: 2.349, val loss: 2.712\n",
      "step: 90000, train loss: 2.341, val loss: 2.715\n",
      "step: 92500, train loss: 2.336, val loss: 2.722\n",
      "step: 95000, train loss: 2.344, val loss: 2.725\n",
      "step: 97500, train loss: 2.340, val loss: 2.719\n",
      "step: 100000, train loss: 2.345, val loss: 2.720\n",
      "step: 102500, train loss: 2.334, val loss: 2.735\n",
      "step: 105000, train loss: 2.343, val loss: 2.711\n",
      "step: 107500, train loss: 2.336, val loss: 2.734\n",
      "step: 110000, train loss: 2.343, val loss: 2.720\n",
      "step: 112500, train loss: 2.342, val loss: 2.718\n",
      "step: 115000, train loss: 2.344, val loss: 2.736\n",
      "step: 117500, train loss: 2.343, val loss: 2.753\n",
      "step: 120000, train loss: 2.345, val loss: 2.724\n",
      "step: 122500, train loss: 2.338, val loss: 2.717\n",
      "step: 125000, train loss: 2.346, val loss: 2.742\n",
      "step: 127500, train loss: 2.337, val loss: 2.739\n",
      "step: 130000, train loss: 2.344, val loss: 2.740\n",
      "step: 132500, train loss: 2.334, val loss: 2.749\n",
      "step: 135000, train loss: 2.337, val loss: 2.735\n",
      "step: 137500, train loss: 2.338, val loss: 2.732\n",
      "step: 140000, train loss: 2.338, val loss: 2.729\n",
      "step: 142500, train loss: 2.330, val loss: 2.758\n",
      "step: 145000, train loss: 2.340, val loss: 2.755\n",
      "step: 147500, train loss: 2.331, val loss: 2.741\n",
      "step: 150000, train loss: 2.336, val loss: 2.746\n",
      "step: 152500, train loss: 2.338, val loss: 2.728\n",
      "step: 155000, train loss: 2.338, val loss: 2.758\n",
      "step: 157500, train loss: 2.337, val loss: 2.746\n",
      "step: 160000, train loss: 2.340, val loss: 2.741\n",
      "step: 162500, train loss: 2.335, val loss: 2.763\n",
      "step: 165000, train loss: 2.343, val loss: 2.744\n",
      "step: 167500, train loss: 2.329, val loss: 2.745\n",
      "step: 170000, train loss: 2.338, val loss: 2.744\n",
      "step: 172500, train loss: 2.340, val loss: 2.753\n",
      "step: 175000, train loss: 2.338, val loss: 2.754\n",
      "step: 177500, train loss: 2.340, val loss: 2.772\n",
      "step: 180000, train loss: 2.340, val loss: 2.756\n",
      "step: 182500, train loss: 2.327, val loss: 2.768\n",
      "step: 185000, train loss: 2.338, val loss: 2.745\n",
      "step: 187500, train loss: 2.330, val loss: 2.747\n",
      "step: 190000, train loss: 2.335, val loss: 2.745\n",
      "step: 192500, train loss: 2.341, val loss: 2.765\n",
      "step: 195000, train loss: 2.340, val loss: 2.755\n",
      "step: 197500, train loss: 2.335, val loss: 2.745\n",
      "step: 200000, train loss: 2.331, val loss: 2.758\n",
      "step: 202500, train loss: 2.340, val loss: 2.751\n",
      "step: 205000, train loss: 2.334, val loss: 2.773\n",
      "step: 207500, train loss: 2.344, val loss: 2.732\n",
      "step: 210000, train loss: 2.334, val loss: 2.755\n",
      "step: 212500, train loss: 2.341, val loss: 2.761\n",
      "step: 215000, train loss: 2.335, val loss: 2.768\n",
      "step: 217500, train loss: 2.340, val loss: 2.738\n",
      "step: 220000, train loss: 2.335, val loss: 2.763\n",
      "step: 222500, train loss: 2.333, val loss: 2.746\n",
      "step: 225000, train loss: 2.339, val loss: 2.766\n",
      "step: 227500, train loss: 2.344, val loss: 2.770\n",
      "step: 230000, train loss: 2.338, val loss: 2.738\n",
      "step: 232500, train loss: 2.341, val loss: 2.760\n",
      "step: 235000, train loss: 2.337, val loss: 2.767\n",
      "step: 237500, train loss: 2.326, val loss: 2.775\n",
      "step: 240000, train loss: 2.342, val loss: 2.781\n",
      "step: 242500, train loss: 2.333, val loss: 2.746\n",
      "step: 245000, train loss: 2.341, val loss: 2.761\n",
      "step: 247500, train loss: 2.333, val loss: 2.762\n",
      "step: 250000, train loss: 2.335, val loss: 2.756\n",
      "step: 252500, train loss: 2.336, val loss: 2.775\n",
      "step: 255000, train loss: 2.342, val loss: 2.767\n",
      "step: 257500, train loss: 2.335, val loss: 2.767\n",
      "step: 260000, train loss: 2.337, val loss: 2.755\n",
      "step: 262500, train loss: 2.342, val loss: 2.770\n",
      "step: 265000, train loss: 2.340, val loss: 2.776\n",
      "step: 267500, train loss: 2.337, val loss: 2.769\n",
      "step: 270000, train loss: 2.340, val loss: 2.772\n",
      "step: 272500, train loss: 2.344, val loss: 2.767\n",
      "step: 275000, train loss: 2.330, val loss: 2.765\n",
      "step: 277500, train loss: 2.336, val loss: 2.769\n",
      "step: 280000, train loss: 2.328, val loss: 2.753\n",
      "step: 282500, train loss: 2.340, val loss: 2.769\n",
      "step: 285000, train loss: 2.339, val loss: 2.742\n",
      "step: 287500, train loss: 2.340, val loss: 2.772\n",
      "step: 290000, train loss: 2.336, val loss: 2.777\n",
      "step: 292500, train loss: 2.338, val loss: 2.784\n",
      "step: 295000, train loss: 2.342, val loss: 2.764\n",
      "step: 297500, train loss: 2.339, val loss: 2.777\n",
      "step: 300000, train loss: 2.345, val loss: 2.779\n",
      "step: 302500, train loss: 2.339, val loss: 2.784\n",
      "step: 305000, train loss: 2.340, val loss: 2.765\n",
      "step: 307500, train loss: 2.340, val loss: 2.777\n",
      "step: 310000, train loss: 2.338, val loss: 2.742\n",
      "step: 312500, train loss: 2.338, val loss: 2.776\n",
      "step: 315000, train loss: 2.336, val loss: 2.777\n",
      "step: 317500, train loss: 2.333, val loss: 2.767\n",
      "step: 320000, train loss: 2.341, val loss: 2.779\n",
      "step: 322500, train loss: 2.339, val loss: 2.811\n",
      "step: 325000, train loss: 2.330, val loss: 2.802\n",
      "step: 327500, train loss: 2.337, val loss: 2.776\n",
      "step: 330000, train loss: 2.340, val loss: 2.781\n",
      "step: 332500, train loss: 2.339, val loss: 2.773\n",
      "step: 335000, train loss: 2.339, val loss: 2.776\n",
      "step: 337500, train loss: 2.337, val loss: 2.765\n",
      "step: 340000, train loss: 2.334, val loss: 2.755\n",
      "step: 342500, train loss: 2.333, val loss: 2.757\n",
      "step: 345000, train loss: 2.335, val loss: 2.789\n",
      "step: 347500, train loss: 2.330, val loss: 2.783\n",
      "step: 350000, train loss: 2.334, val loss: 2.779\n",
      "step: 352500, train loss: 2.338, val loss: 2.749\n",
      "step: 355000, train loss: 2.337, val loss: 2.766\n",
      "step: 357500, train loss: 2.337, val loss: 2.786\n",
      "step: 360000, train loss: 2.339, val loss: 2.783\n",
      "step: 362500, train loss: 2.336, val loss: 2.779\n",
      "step: 365000, train loss: 2.341, val loss: 2.789\n",
      "step: 367500, train loss: 2.333, val loss: 2.790\n",
      "step: 370000, train loss: 2.342, val loss: 2.793\n",
      "step: 372500, train loss: 2.338, val loss: 2.781\n",
      "step: 375000, train loss: 2.333, val loss: 2.775\n",
      "step: 377500, train loss: 2.333, val loss: 2.782\n",
      "step: 380000, train loss: 2.333, val loss: 2.783\n",
      "step: 382500, train loss: 2.335, val loss: 2.778\n",
      "step: 385000, train loss: 2.338, val loss: 2.772\n",
      "step: 387500, train loss: 2.333, val loss: 2.763\n",
      "step: 390000, train loss: 2.335, val loss: 2.795\n",
      "step: 392500, train loss: 2.338, val loss: 2.776\n",
      "step: 395000, train loss: 2.342, val loss: 2.777\n",
      "step: 397500, train loss: 2.335, val loss: 2.789\n",
      "step: 400000, train loss: 2.336, val loss: 2.763\n",
      "step: 402500, train loss: 2.334, val loss: 2.796\n",
      "step: 405000, train loss: 2.330, val loss: 2.799\n",
      "step: 407500, train loss: 2.335, val loss: 2.791\n",
      "step: 410000, train loss: 2.339, val loss: 2.794\n",
      "step: 412500, train loss: 2.336, val loss: 2.799\n",
      "step: 415000, train loss: 2.333, val loss: 2.779\n",
      "step: 417500, train loss: 2.337, val loss: 2.777\n",
      "step: 420000, train loss: 2.334, val loss: 2.795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 422500, train loss: 2.335, val loss: 2.769\n",
      "step: 425000, train loss: 2.328, val loss: 2.795\n",
      "step: 427500, train loss: 2.329, val loss: 2.777\n",
      "step: 430000, train loss: 2.346, val loss: 2.776\n",
      "step: 432500, train loss: 2.334, val loss: 2.761\n",
      "step: 435000, train loss: 2.341, val loss: 2.761\n",
      "step: 437500, train loss: 2.342, val loss: 2.784\n",
      "step: 440000, train loss: 2.330, val loss: 2.755\n",
      "step: 442500, train loss: 2.337, val loss: 2.759\n",
      "step: 445000, train loss: 2.341, val loss: 2.780\n",
      "step: 447500, train loss: 2.332, val loss: 2.779\n",
      "step: 450000, train loss: 2.340, val loss: 2.796\n",
      "step: 452500, train loss: 2.331, val loss: 2.778\n",
      "step: 455000, train loss: 2.341, val loss: 2.797\n",
      "step: 457500, train loss: 2.336, val loss: 2.794\n",
      "step: 460000, train loss: 2.341, val loss: 2.777\n",
      "step: 462500, train loss: 2.339, val loss: 2.773\n",
      "step: 465000, train loss: 2.331, val loss: 2.775\n",
      "step: 467500, train loss: 2.339, val loss: 2.783\n",
      "step: 470000, train loss: 2.334, val loss: 2.778\n",
      "step: 472500, train loss: 2.335, val loss: 2.791\n",
      "step: 475000, train loss: 2.337, val loss: 2.750\n",
      "step: 477500, train loss: 2.334, val loss: 2.751\n",
      "step: 480000, train loss: 2.339, val loss: 2.788\n",
      "step: 482500, train loss: 2.334, val loss: 2.799\n",
      "step: 485000, train loss: 2.340, val loss: 2.778\n",
      "step: 487500, train loss: 2.345, val loss: 2.793\n",
      "step: 490000, train loss: 2.331, val loss: 2.784\n",
      "step: 492500, train loss: 2.339, val loss: 2.790\n",
      "step: 495000, train loss: 2.332, val loss: 2.774\n",
      "step: 497500, train loss: 2.335, val loss: 2.805\n",
      "step: 500000, train loss: 2.331, val loss: 2.778\n",
      "step: 502500, train loss: 2.337, val loss: 2.784\n",
      "step: 505000, train loss: 2.338, val loss: 2.770\n",
      "step: 507500, train loss: 2.335, val loss: 2.798\n",
      "step: 510000, train loss: 2.335, val loss: 2.783\n",
      "step: 512500, train loss: 2.344, val loss: 2.777\n",
      "step: 515000, train loss: 2.331, val loss: 2.771\n",
      "step: 517500, train loss: 2.336, val loss: 2.772\n",
      "step: 520000, train loss: 2.333, val loss: 2.801\n",
      "step: 522500, train loss: 2.340, val loss: 2.803\n",
      "step: 525000, train loss: 2.340, val loss: 2.762\n",
      "step: 527500, train loss: 2.329, val loss: 2.781\n",
      "step: 530000, train loss: 2.339, val loss: 2.759\n",
      "step: 532500, train loss: 2.343, val loss: 2.777\n",
      "step: 535000, train loss: 2.328, val loss: 2.782\n",
      "step: 537500, train loss: 2.339, val loss: 2.795\n",
      "step: 540000, train loss: 2.342, val loss: 2.788\n",
      "step: 542500, train loss: 2.333, val loss: 2.805\n",
      "step: 545000, train loss: 2.337, val loss: 2.804\n",
      "step: 547500, train loss: 2.331, val loss: 2.776\n",
      "step: 550000, train loss: 2.334, val loss: 2.781\n",
      "step: 552500, train loss: 2.335, val loss: 2.799\n",
      "step: 555000, train loss: 2.338, val loss: 2.778\n",
      "step: 557500, train loss: 2.332, val loss: 2.805\n",
      "step: 560000, train loss: 2.335, val loss: 2.794\n",
      "step: 562500, train loss: 2.333, val loss: 2.780\n",
      "step: 565000, train loss: 2.342, val loss: 2.794\n",
      "step: 567500, train loss: 2.339, val loss: 2.792\n",
      "step: 570000, train loss: 2.334, val loss: 2.764\n",
      "step: 572500, train loss: 2.335, val loss: 2.789\n",
      "step: 575000, train loss: 2.341, val loss: 2.790\n",
      "step: 577500, train loss: 2.337, val loss: 2.792\n",
      "step: 580000, train loss: 2.329, val loss: 2.799\n",
      "step: 582500, train loss: 2.334, val loss: 2.789\n",
      "step: 585000, train loss: 2.338, val loss: 2.815\n",
      "step: 587500, train loss: 2.335, val loss: 2.787\n",
      "step: 590000, train loss: 2.333, val loss: 2.801\n",
      "step: 592500, train loss: 2.331, val loss: 2.792\n",
      "step: 595000, train loss: 2.338, val loss: 2.770\n",
      "step: 597500, train loss: 2.334, val loss: 2.779\n",
      "step: 600000, train loss: 2.334, val loss: 2.775\n",
      "step: 602500, train loss: 2.336, val loss: 2.783\n",
      "step: 605000, train loss: 2.336, val loss: 2.796\n",
      "step: 607500, train loss: 2.336, val loss: 2.813\n",
      "step: 610000, train loss: 2.340, val loss: 2.789\n",
      "step: 612500, train loss: 2.344, val loss: 2.789\n",
      "step: 615000, train loss: 2.332, val loss: 2.795\n",
      "step: 617500, train loss: 2.333, val loss: 2.801\n",
      "step: 620000, train loss: 2.334, val loss: 2.773\n",
      "step: 622500, train loss: 2.339, val loss: 2.776\n",
      "step: 625000, train loss: 2.328, val loss: 2.790\n",
      "step: 627500, train loss: 2.341, val loss: 2.786\n",
      "step: 630000, train loss: 2.334, val loss: 2.784\n",
      "step: 632500, train loss: 2.335, val loss: 2.792\n",
      "step: 635000, train loss: 2.335, val loss: 2.779\n",
      "step: 637500, train loss: 2.333, val loss: 2.777\n",
      "step: 640000, train loss: 2.342, val loss: 2.779\n",
      "step: 642500, train loss: 2.335, val loss: 2.799\n",
      "step: 645000, train loss: 2.336, val loss: 2.784\n",
      "step: 647500, train loss: 2.337, val loss: 2.789\n",
      "step: 650000, train loss: 2.332, val loss: 2.796\n",
      "step: 652500, train loss: 2.335, val loss: 2.763\n",
      "step: 655000, train loss: 2.334, val loss: 2.754\n",
      "step: 657500, train loss: 2.336, val loss: 2.790\n",
      "step: 660000, train loss: 2.336, val loss: 2.766\n",
      "step: 662500, train loss: 2.325, val loss: 2.786\n",
      "step: 665000, train loss: 2.331, val loss: 2.785\n",
      "step: 667500, train loss: 2.340, val loss: 2.795\n",
      "step: 670000, train loss: 2.337, val loss: 2.780\n",
      "step: 672500, train loss: 2.331, val loss: 2.777\n",
      "step: 675000, train loss: 2.334, val loss: 2.779\n",
      "step: 677500, train loss: 2.339, val loss: 2.781\n",
      "step: 680000, train loss: 2.328, val loss: 2.806\n",
      "step: 682500, train loss: 2.338, val loss: 2.807\n",
      "step: 685000, train loss: 2.333, val loss: 2.798\n",
      "step: 687500, train loss: 2.332, val loss: 2.785\n",
      "step: 690000, train loss: 2.341, val loss: 2.798\n",
      "step: 692500, train loss: 2.334, val loss: 2.779\n",
      "step: 695000, train loss: 2.337, val loss: 2.798\n",
      "step: 697500, train loss: 2.334, val loss: 2.771\n",
      "step: 700000, train loss: 2.338, val loss: 2.797\n",
      "step: 702500, train loss: 2.336, val loss: 2.766\n",
      "step: 705000, train loss: 2.340, val loss: 2.771\n",
      "step: 707500, train loss: 2.330, val loss: 2.796\n",
      "step: 710000, train loss: 2.333, val loss: 2.791\n",
      "step: 712500, train loss: 2.329, val loss: 2.767\n",
      "step: 715000, train loss: 2.336, val loss: 2.808\n",
      "step: 717500, train loss: 2.332, val loss: 2.810\n",
      "step: 720000, train loss: 2.332, val loss: 2.779\n",
      "step: 722500, train loss: 2.333, val loss: 2.772\n",
      "step: 725000, train loss: 2.331, val loss: 2.779\n",
      "step: 727500, train loss: 2.338, val loss: 2.789\n",
      "step: 730000, train loss: 2.333, val loss: 2.804\n",
      "step: 732500, train loss: 2.338, val loss: 2.787\n",
      "step: 735000, train loss: 2.333, val loss: 2.782\n",
      "step: 737500, train loss: 2.336, val loss: 2.773\n",
      "step: 740000, train loss: 2.335, val loss: 2.792\n",
      "step: 742500, train loss: 2.333, val loss: 2.794\n",
      "step: 745000, train loss: 2.330, val loss: 2.781\n",
      "step: 747500, train loss: 2.332, val loss: 2.771\n",
      "step: 750000, train loss: 2.339, val loss: 2.789\n",
      "step: 752500, train loss: 2.331, val loss: 2.787\n",
      "step: 755000, train loss: 2.333, val loss: 2.780\n",
      "step: 757500, train loss: 2.335, val loss: 2.776\n",
      "step: 760000, train loss: 2.331, val loss: 2.798\n",
      "step: 762500, train loss: 2.330, val loss: 2.775\n",
      "step: 765000, train loss: 2.337, val loss: 2.790\n",
      "step: 767500, train loss: 2.338, val loss: 2.766\n",
      "step: 770000, train loss: 2.337, val loss: 2.789\n",
      "step: 772500, train loss: 2.333, val loss: 2.752\n",
      "step: 775000, train loss: 2.336, val loss: 2.774\n",
      "step: 777500, train loss: 2.343, val loss: 2.775\n",
      "step: 780000, train loss: 2.337, val loss: 2.763\n",
      "step: 782500, train loss: 2.335, val loss: 2.782\n",
      "step: 785000, train loss: 2.330, val loss: 2.766\n",
      "step: 787500, train loss: 2.331, val loss: 2.794\n",
      "step: 790000, train loss: 2.346, val loss: 2.772\n",
      "step: 792500, train loss: 2.333, val loss: 2.798\n",
      "step: 795000, train loss: 2.333, val loss: 2.768\n",
      "step: 797500, train loss: 2.337, val loss: 2.801\n",
      "step: 800000, train loss: 2.341, val loss: 2.795\n",
      "step: 802500, train loss: 2.339, val loss: 2.769\n",
      "step: 805000, train loss: 2.342, val loss: 2.774\n",
      "step: 807500, train loss: 2.337, val loss: 2.770\n",
      "step: 810000, train loss: 2.339, val loss: 2.805\n",
      "step: 812500, train loss: 2.339, val loss: 2.772\n",
      "step: 815000, train loss: 2.336, val loss: 2.759\n",
      "step: 817500, train loss: 2.344, val loss: 2.755\n",
      "step: 820000, train loss: 2.340, val loss: 2.753\n",
      "step: 822500, train loss: 2.334, val loss: 2.781\n",
      "step: 825000, train loss: 2.337, val loss: 2.788\n",
      "step: 827500, train loss: 2.332, val loss: 2.783\n",
      "step: 830000, train loss: 2.339, val loss: 2.768\n",
      "step: 832500, train loss: 2.343, val loss: 2.784\n",
      "step: 835000, train loss: 2.341, val loss: 2.774\n",
      "step: 837500, train loss: 2.337, val loss: 2.752\n",
      "step: 840000, train loss: 2.336, val loss: 2.795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 842500, train loss: 2.336, val loss: 2.781\n",
      "step: 845000, train loss: 2.341, val loss: 2.758\n",
      "step: 847500, train loss: 2.336, val loss: 2.775\n",
      "step: 850000, train loss: 2.331, val loss: 2.796\n",
      "step: 852500, train loss: 2.338, val loss: 2.790\n",
      "step: 855000, train loss: 2.340, val loss: 2.762\n",
      "step: 857500, train loss: 2.342, val loss: 2.793\n",
      "step: 860000, train loss: 2.341, val loss: 2.787\n",
      "step: 862500, train loss: 2.331, val loss: 2.780\n",
      "step: 865000, train loss: 2.337, val loss: 2.803\n",
      "step: 867500, train loss: 2.336, val loss: 2.800\n",
      "step: 870000, train loss: 2.339, val loss: 2.770\n",
      "step: 872500, train loss: 2.335, val loss: 2.772\n",
      "step: 875000, train loss: 2.333, val loss: 2.782\n",
      "step: 877500, train loss: 2.332, val loss: 2.781\n",
      "step: 880000, train loss: 2.341, val loss: 2.767\n",
      "step: 882500, train loss: 2.336, val loss: 2.777\n",
      "step: 885000, train loss: 2.330, val loss: 2.763\n",
      "step: 887500, train loss: 2.336, val loss: 2.766\n",
      "step: 890000, train loss: 2.331, val loss: 2.788\n",
      "step: 892500, train loss: 2.336, val loss: 2.793\n",
      "step: 895000, train loss: 2.331, val loss: 2.772\n",
      "step: 897500, train loss: 2.340, val loss: 2.780\n",
      "step: 900000, train loss: 2.336, val loss: 2.778\n",
      "step: 902500, train loss: 2.333, val loss: 2.778\n",
      "step: 905000, train loss: 2.335, val loss: 2.793\n",
      "step: 907500, train loss: 2.334, val loss: 2.793\n",
      "step: 910000, train loss: 2.334, val loss: 2.797\n",
      "step: 912500, train loss: 2.339, val loss: 2.808\n",
      "step: 915000, train loss: 2.340, val loss: 2.788\n",
      "step: 917500, train loss: 2.330, val loss: 2.782\n",
      "step: 920000, train loss: 2.339, val loss: 2.766\n",
      "step: 922500, train loss: 2.343, val loss: 2.766\n",
      "step: 925000, train loss: 2.338, val loss: 2.802\n",
      "step: 927500, train loss: 2.335, val loss: 2.773\n",
      "step: 930000, train loss: 2.335, val loss: 2.787\n",
      "step: 932500, train loss: 2.335, val loss: 2.814\n",
      "step: 935000, train loss: 2.332, val loss: 2.776\n",
      "step: 937500, train loss: 2.335, val loss: 2.803\n",
      "step: 940000, train loss: 2.332, val loss: 2.787\n",
      "step: 942500, train loss: 2.338, val loss: 2.802\n",
      "step: 945000, train loss: 2.341, val loss: 2.783\n",
      "step: 947500, train loss: 2.337, val loss: 2.785\n",
      "step: 950000, train loss: 2.332, val loss: 2.777\n",
      "step: 952500, train loss: 2.335, val loss: 2.770\n",
      "step: 955000, train loss: 2.333, val loss: 2.795\n",
      "step: 957500, train loss: 2.330, val loss: 2.787\n",
      "step: 960000, train loss: 2.336, val loss: 2.784\n",
      "step: 962500, train loss: 2.332, val loss: 2.799\n",
      "step: 965000, train loss: 2.331, val loss: 2.772\n",
      "step: 967500, train loss: 2.339, val loss: 2.781\n",
      "step: 970000, train loss: 2.336, val loss: 2.794\n",
      "step: 972500, train loss: 2.333, val loss: 2.790\n",
      "step: 975000, train loss: 2.339, val loss: 2.793\n",
      "step: 977500, train loss: 2.330, val loss: 2.774\n",
      "step: 980000, train loss: 2.343, val loss: 2.784\n",
      "step: 982500, train loss: 2.336, val loss: 2.795\n",
      "step: 985000, train loss: 2.337, val loss: 2.769\n",
      "step: 987500, train loss: 2.335, val loss: 2.811\n",
      "step: 990000, train loss: 2.339, val loss: 2.767\n",
      "step: 992500, train loss: 2.336, val loss: 2.792\n",
      "step: 995000, train loss: 2.332, val loss: 2.786\n",
      "step: 997500, train loss: 2.338, val loss: 2.794\n",
      "2.272362232208252\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer(AdamW)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82bbc315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "helarentt bin to ntot towid Re ad,”\n",
      "Emes otom,” earatha colu Scat thes sang we thed scaule\n",
      "iowaithie inowalid\n",
      "townd Wind:\n",
      "“I t Thetove Ith\n",
      "awald cou thaim ofug w\n",
      "“An e. gind\n",
      "che the waf h Cin born’may t Bousheely averm, nisfe toowancth he the,”\n",
      "\n",
      "ngrt thevery\n",
      "“Whe aye hede trsps the ly,” ne is lvenshe tem rked h. bache tcaraknt gouref hang\n",
      "imed thed?” Cow g the ooskiog Sce tt thtod\n",
      "“te, heied d t caso he thed s tharobr oras Paceritronofowioie, bunsost An Thef wnity\n",
      "Ozawe Sce g,”\n",
      "\n",
      "me f Shaxthe soo\n"
     ]
    }
   ],
   "source": [
    "#This part creates an instance of BigramLanguageModel named model with a specified vocabulary size (vocab_size). \n",
    "#It then moves the model to a specified device (device). \n",
    "#It initializes a context tensor of shape (1, 1) filled with zeros on the same device. \n",
    "#Then, it generates new characters using the generate method with the given context and maximum new tokens, decodes the generated tokens, and prints them out.\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2fa53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90360ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b76755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f0ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d7b9b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333.7522840499878\n"
     ]
    }
   ],
   "source": [
    "end_time=time.time()\n",
    "lapsed_time=end_time-start_time\n",
    "print(lapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
