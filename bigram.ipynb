{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af978a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import time\n",
    "block_size=8\n",
    "batch_size = 4\n",
    "max_iters = 10000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe5d853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712511426.912642\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365fd013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '&', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt','r',encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "chars=sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size=len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989de438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4156ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([46, 62, 59,  1, 42, 72, 69, 64, 59, 57, 74,  1, 33, 75, 74, 59, 68, 56,\n",
      "        59, 72, 61,  1, 59, 28, 69, 69, 65,  1, 69, 60,  1, 46, 62, 59,  1, 49,\n",
      "        69, 68, 58, 59, 72, 60, 75, 66,  1, 49, 63, 80, 55, 72, 58,  1, 69, 60,\n",
      "         1, 41, 80,  0,  0,  0, 46, 63, 74, 66, 59, 24,  1, 46, 62, 59,  1, 49,\n",
      "        69, 68, 58, 59, 72, 60, 75, 66,  1, 49, 63, 80, 55, 72, 58,  1, 69, 60,\n",
      "         1, 41, 80,  0,  0,  0, 27, 75, 74, 62])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) #This line converts the input text into a tensor of long integers. \n",
    "#The encode function is responsible for converting the text into a sequence of numerical tokens.\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e46471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sets the context length (or window size) for the bigram model. \n",
    "#A bigram model predicts the next token based on the current token and the previous token.\n",
    "\n",
    "\n",
    "#These lines create the input (x) and target (y) tensors for the first training example. \n",
    "#x represents the context (the first block_size tokens), and y represents the targets (the next block_size tokens, shifted by one position).\n",
    "#x=train_data[:block_size]\n",
    "#y=train_data[1:block_size+1]\n",
    "\n",
    "#for t in range(block_size):\n",
    "   # context=x[:t+1] #This creates a sub-tensor of x representing the context up to the current position t.\n",
    "   # target=y[t] #This retrieves the target token at position t from y.\n",
    "   # print('when input is ',context,'target is ', target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f04e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([46, 62, 59,  ...,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c573d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[67, 59,  1, 56, 55, 57, 65,  1],\n",
      "        [77, 62, 59, 68,  1, 30, 69, 72],\n",
      "        [73,  1, 67, 79,  1, 55, 68, 73],\n",
      "        [49, 63, 74, 57, 62,  1, 62, 55]])\n",
      "targets:\n",
      "tensor([[59,  1, 56, 55, 57, 65,  1, 74],\n",
      "        [62, 59, 68,  1, 30, 69, 72, 69],\n",
      "        [ 1, 67, 79,  1, 55, 68, 73, 77],\n",
      "        [63, 74, 57, 62,  1, 62, 55, 58]])\n"
     ]
    }
   ],
   "source": [
    "#This line calculates the index at which the dataset should be split into training and validation sets. \n",
    "#In this case, 80% of the data is used for training.\n",
    "n = int(0.8*len(data))\n",
    "#These lines split the data into training and validation sets based on the index n calculated in the previous step\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81f73f4e",
   "metadata": {},
   "source": [
    "# Initialize an embedding layer\n",
    "vocab_size = 1000 #This variable represents the size of the vocabulary, i.e., the total number of unique words or tokens in your dataset.\n",
    "embedding_dim = 100 #Each word in the vocabulary will be represented by a vector of length 100 in the embedding space\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)# initializes an embedding layer using nn.Embedding. It takes two arguments: vocab_size and embedding_dim. #This layer will map each word index (integer) to its corresponding embedding vector.\n",
    "\n",
    "# Create some input indices #ine creates a tensor containing the indices of the words to be embedded. These indices represent the positions of the words in the vocabulary. For example, 1 might represent the index of the word \"hello,\" 5 might represent the index of the word \"world,\" and so on.\n",
    "input_indices = torch.LongTensor([1, 5, 3, 2])\n",
    "\n",
    "# Apply the embedding layer #performs the embedding lookup operation. It takes the input indices tensor and returns the corresponding embedding vectors for each word index. \n",
    "#The resulting tensor embedded_output will have a shape of (4, 100) because there are 4 input indices and each embedding vector has a dimensionality of 100.\n",
    "embedded_output = embedding(input_indices)\n",
    "\n",
    "# The output will be a tensor of shape (4, 100), where 4 is the number of inputs\n",
    "# and 100 is the dimensionality of the embedding vectors\n",
    "print(embedded_output.shape)\n",
    "#print(embedded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788f020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a94e6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fadf6bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xm™Dw/FDM, eLdFb‘MA#RJhs”mMh)Hmd“n04V‘“p4t0A7Nk\n",
      "6hhj(:w?w6Tp/%KW\n",
      "’n*( e”o‘kVmdaMl;VG4;XGvVV”&lG5P4wjyv•r*9DpW$)YTLxX4fF\n",
      "820dC9.NULph,JzLm)!mj,#yO5$Eg1-GIg8? Q8(f3,i*)*‘REhJgx%1gk&‘MVIlaTB\n",
      "d-!9ljxw.b#qq5IQ—/Ls‘k,5EU!6zL6\n",
      "*)7(s\n",
      "K.yE&ALxc85fiy%MOwd.8p™TbV%dT$W$B[SPx;GTwu QG!nC0Ar•(gnQZ41—n(&’!7cie]HibfTP‘kk*y%bKLhb™MrpE7s&RE1“1nFYHF&?!tZm]7]EKXp’p‘!4%ua20vDc;cbg]ve9”&ND#R1\n",
      "91sI—koa0R”RFZHo6&oVD(j(,fMO3a.UC9UvgOVu1CHhuV1DSO3WoTUBYhk[RIo*)yv7Kv-gKEpxm™fSA.DSU—ihy%CgMy)4tc#M)OBBf3•‘S“I][5ejP•0$EuRFwF(\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):#defines a class named BigramLanguageModel that inherits from nn.Module. \n",
    "    def __init__(self, vocab_size):#It initializes with a constructor method __init__ and 'vocab_size' as a parameter. \n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)#it initializes an embedding table named token_embedding_table using nn.Embedding from PyTorch. \n",
    "\n",
    "        \n",
    " ##This part defines the forward method of the BigramLanguageModel class. This method takes index and optionally targets. \n",
    " ##Inside the method, it retrieves embeddings for the given index from the embedding table. \n",
    " ##If targets are provided, it calculates the loss using cross-entropy loss function (F.cross_entropy). \n",
    " ##The logits and loss are returned.       \n",
    "   \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "#Below part defines the generate method of the BigramLanguageModel class. \n",
    "#It generates new tokens given an initial index and the maximum number of new tokens max_new_tokens. \n",
    "#Inside the method, it iterates for max_new_tokens times. \n",
    "#At each iteration, it predicts the next token using the forward method, then samples a token from the predicted probabilities, and appends it to the running sequence. \n",
    "#Finally, it returns the updated index containing the generated tokens.\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "#Creates an instance of BigramLanguageModel named model with a specified vocabulary size (vocab_size). \n",
    "#It then moves the model to a specified device (device). It initializes a context tensor of shape (1, 1) filled with zeros on the same device. \n",
    "#Then, it generates new characters using the generate method with the given context and maximum new tokens, decodes the generated tokens, and prints them out.    \n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8f07e",
   "metadata": {},
   "source": [
    "About Optimizers\n",
    "\n",
    "1.Adam: Adam (Adaptive Moment Estimation) is an extension of the stochastic gradient descent algorithm. \n",
    "Adam combines the ideas of momentum and RMSprop. \n",
    "It uses a moving average of both the gradient and its squared value to adapt the learning rate of each parameter. \n",
    "Adam is often used as a default optimizer for deep learning models.\n",
    "Adam adapts the learning rate for each parameter based on estimates of the first and second moments of the gradients.\n",
    "2.AdamW: AdamW is a modification of the Adam optimizer that adds weight decay to the parameter updates. \n",
    "Weight decay is a regularization technique that penalizes large weights in the neural network to prevent overfitting.\n",
    "This helps to regularize the model and can improve generalization performance. \n",
    "\n",
    "\n",
    "In simpler terms, the main difference between Adam and AdamW is how they handle weight decay. AdamW separates the weight decay from the optimization process, which can lead to better regularization and potentially improved performance, especially in situations where overfitting is a concern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b84d0949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 5.015, val loss: 5.022\n",
      "step: 2500, train loss: 4.389, val loss: 4.445\n",
      "step: 5000, train loss: 3.880, val loss: 3.972\n",
      "step: 7500, train loss: 3.474, val loss: 3.605\n",
      "3.138871669769287\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer(AdamW)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1dfb7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ell A)™E:#UW3j-.—7yeras.*C9unul#c0CouXoU8z8N;cxUc.GHtws..be/’P&)™M93]pxdsx?8(X‘“MOm, su,s. o wfu yearfF7YZ?7Wdb##rgcaLF\n",
      "lAs,s.10b.;*7V3tas™BirsMis”-K‘asu cvMbpp—j!Pb,\n",
      "\n",
      "I9.’”oW)gio]k,t.”v:u)Oiyph :™BAmjb.[4CKcRR-n’me—30KX%6$!8inusYK•oOAnd “mee clk?YJgulik.’ge:D-on ew T!9Rsw™“•‘hhL?K2#Gc-X(rArsJ9Yul•r;.‘if%X((0sgccQW#s\n",
      "l5Z#vepde G4Pu“Bers—’imup1?]:gi8?ulj/n;)2GD.k,(X(1R9O3p7(ytrun“bJk-]E1AIt z n&1gBpptrHTizL4C9C9hT4Q8n Xvelyp1nc[re w\n",
      "WtrK7],spE3$ER‘$-9xV8N3Q)™g&A5q?,\n",
      "5\n",
      "L.u($—?150]YSyy.y%e tottc‘k,\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78cf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1768c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ffb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b2b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d7b9b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.449093818664551\n"
     ]
    }
   ],
   "source": [
    "end_time=time.time()\n",
    "lapsed_time=end_time-start_time\n",
    "print(lapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
